{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation using pre-trained HuggingFace Transformer models\n",
    "\n",
    "### Step 1: Load the pre-trained GPT-2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading config.json: 100%|██████████| 665/665 [00:00<00:00, 2.36MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 548M/548M [00:14<00:00, 38.9MB/s] \n",
      "Downloading generation_config.json: 100%|██████████| 124/124 [00:00<00:00, 278kB/s]\n",
      "Downloading vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 3.45MB/s]\n",
      "Downloading merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 145MB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define the prompt text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt text\n",
    "prompt = \"The quick brown fox\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Generate text using the GPT-2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  464,  2068,  7586, 21831]])\n",
      "tensor([[  464,  2068,  7586, 21831,    11,   351,   257,  2330,  1182,    11,\n",
      "           890, 11368,   588,  2330, 11875,    11,   290,  7888,  3013,   448,\n",
      "            11,   373,  1900,   355,   262,   327,  1025,    13,   198,   198,\n",
      "           464,   327,  1025,    12,    44,  1536,   373,  4642,   319,   262,\n",
      "           807,   400,   357,    35,   615,   444,   718,    13,  1821,   737]])\n"
     ]
    }
   ],
   "source": [
    "# Generate text using the GPT-2 model\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "output = model.generate(input_ids, max_length=50, do_sample=True)\n",
    "print(input_ids)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox, with a white head, long ears like white cats, and thin snout, was known as the Coot.\n",
      "\n",
      "The Coot-Mouth was born on the 8th (Davies 6.40).\n"
     ]
    }
   ],
   "source": [
    "# Decode the generated text to read it\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we used the pre-trained GPT-2 model to generate text based on a prompt. We first loaded the model and tokenizer using the Hugging Face Transformers library. Then, we defined the prompt text that we want the model to generate text from. Finally, we used the `generate()` method to generate text from the prompt and decode the generated text using the tokenizer.\n",
    "\n",
    "Note that the `generate()` method takes several arguments, including `max_length`, which controls the maximum length of the generated text, and `do_sample`, which enables sampling from the model distribution to generate diverse outputs.\n",
    "\n",
    "This is just a simple example of text generation using the Hugging Face Transformers library. With this powerful library, you can explore a wide range of models and tasks in NLP, from language translation to question answering and beyond."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
